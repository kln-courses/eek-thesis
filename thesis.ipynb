{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files sorted on filename ##\n",
    "\n",
    "Here we import your .txt files and their filenames. We sort the filenames to keep the order of your index.\n",
    "\n",
    "nb. you need to install the `natsort` module: https://anaconda.org/anaconda/natsort\n",
    "    , which you can either do from the Anaconda Prompt or from the Anaconda Navigator.\n",
    "    If you use the prompt write: `conda install natsort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The people of falsehood constantly attempt to make the deaths of righteous men and their slayings by the enemies of Islam – the mushrikin and the apostates – into a sign foretelling the breaking of the muwahiddin. But those fools do not realize that Allah has ordained for each soul its set term before He created the heavens and the earth. Allah said, “And each nation has its set term. They can neither delay it for an hour nor advance it” (Al-A’raf 34). In this decree, all people are equal, including prophets and righteous people as well as disbelievers and tyrants. Those fools do not realize that Allah preserves His religion however He wills, and this religion will remain established and will not be damaged by the death of any person. If it would have been damaged by anything, it would have been by the death of the Prophet and those of his noble Companions. But the religion remained long after their departure, as Allah established its foothold and spread it on the earth. He preserved i\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "from natsort import natsorted\n",
    "\n",
    "def read_txt_dir(dirpath):\n",
    "    \"\"\" import all .txt files from directory of directory path dirpath\n",
    "        - output file and filename in list\n",
    "    \"\"\"\n",
    "    filenames = natsorted(glob.glob(os.path.join(dirpath,\"*.txt\")))\n",
    "    files = list()\n",
    "    for filename in filenames:\n",
    "        with open(filename,\"r\") as fobj:\n",
    "            files.append(fobj.read())\n",
    "    filenames = [filename.split(\"/\")[-1] for filename in filenames]\n",
    "    return files, filenames \n",
    "\n",
    "# import articles\n",
    "article_path = os.path.join(\"dat\",\"articles\")\n",
    "articles, article_names = read_txt_dir(article_path)\n",
    "\n",
    "# import magazines\n",
    "magazine_path = os.path.join(\"dat\",\"magazines\")\n",
    "magazines, magazine_names = read_txt_dir(magazine_path)\n",
    "\n",
    "print(magazines[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and basic corpus statistics ##\n",
    "\n",
    "Start by computing basic corpus statistics. For this you need several functions for preprocessing your string data. We use `re` to remove punctuation and `NLTK` for tokenization. The functionality can be implemented with `re` alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus consist of 287177 tokens distributed over 13104 lexical types\n",
      "The lexical richness measured as the type-token ratio is 0.0456\n",
      "On average every word is repeated 21.92 times\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    pattern = re.compile(r\"\\W+\")\n",
    "    return pattern.sub(\"\",string)\n",
    "\n",
    "def clean_word_tokenize(string, casefold = True, nopunct = True):\n",
    "    \"\"\" Word-level tokenizer that casefolds to lower if True\n",
    "    and removes punctuation if True\n",
    "    \"\"\"\n",
    "    if casefold:\n",
    "        string = string.lower()\n",
    "    else:\n",
    "        pass\n",
    "    tokens = word_tokenize(string)\n",
    "    if nopunct:\n",
    "        tokens = [remove_punctuation(token) for token in tokens]\n",
    "    else:\n",
    "        pass\n",
    "    tokens = [token for token in tokens if token]\n",
    "    return tokens\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# tokenize all articles at word level (all words are lowercased and punctuation removed)\n",
    "articles_tokens = list(map(clean_word_tokenize,articles))# apply tokenizer to all string object in list\n",
    "tokens = sorted(flatten(articles_tokens))# all tokens in one (flat) sorted list\n",
    "n_tokens = len(tokens)# number of tokens\n",
    "n_types = len(list(set(tokens)))# number of typeshhh\n",
    "\n",
    "print(\"The corpus consist of {} tokens distributed over {} lexical types\".format(n_tokens, n_types))\n",
    "print(\"The lexical richness measured as the type-token ratio is {}\".format(round(n_types/n_tokens,4)))\n",
    "print(\"On average every word is repeated {} times\".format(round(n_tokens/n_types,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword filtering in list of lists ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of lists with stopwords:\n",
      "\n",
      "[['he', 'said', 'he', 'loved', 'good', 'food'], ['she', 'and', 'allah', 'had', 'great', 'fun'], ['so', 'what', 'am', 'i', 'going', 'to', 'do']]\n",
      "list of lists without stopwords:\n",
      "\n",
      "[['loved', 'good', 'food'], ['she', 'had', 'great', 'fun'], ['am', 'going', 'do']]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "def gen_ls_stoplist(input, n = 100):\n",
    "    t_f_total = defaultdict(int)\n",
    "    for text in input:\n",
    "        for token in text:\n",
    "            t_f_total[token] += 1\n",
    "    nmax = sorted(t_f_total.items(), key = itemgetter(1), reverse = True)[:n]\n",
    "    return [elem[0] for elem in nmax]\n",
    "\n",
    "# generate stopword list from articles with 50 stopwords (not really a good sw list)\n",
    "sw = gen_ls_stoplist(articles_tokens, 50)\n",
    "\n",
    "# list with three texts in string\n",
    "listofstrings = [\"he said he loved good food\",\"she and allah had great fun\",\"so what am I going to do\"]\n",
    "\n",
    "# tokenize each text\n",
    "listoflists = list()\n",
    "for string in listofstrings:\n",
    "    listoflists.append(clean_word_tokenize(string))\n",
    "\n",
    "print(\"list of lists with stopwords:\\n\")\n",
    "print(listoflists)\n",
    "\n",
    "# remove stopwords from list of list\n",
    "nostopwords = list()\n",
    "for alist in listoflists:\n",
    "    out = [token for token in alist if token not in sw]\n",
    "    nostopwords.append(out)\n",
    "\n",
    "print(\"list of lists without stopwords:\\n\")\n",
    "print(nostopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "def gen_ls_stoplist(input, n = 100):\n",
    "    t_f_total = defaultdict(int)\n",
    "    #n = 100\n",
    "    for text in input:\n",
    "        for token in text:\n",
    "            t_f_total[token] += 1\n",
    "    nmax = sorted(t_f_total.items(), key = itemgetter(1), reverse = True)[:n]\n",
    "    return [elem[0] for elem in nmax]\n",
    "\n",
    "# generate stopword list from articles with 50 stopwords (not really a good sw list)\n",
    "sw = gen_ls_stoplist(articles_tokens, 50)\n",
    "\n",
    "\n",
    "# import sw list from nltk instead\n",
    "\n",
    "import io\n",
    "\n",
    "def read_txt(filepath):\n",
    "    \"\"\"\n",
    "    Read txt file from filepath and returns char content\n",
    "    in string\n",
    "    \"\"\"\n",
    "    f = io.open(filepath, 'r', encoding = 'utf-8')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    return content\n",
    "\n",
    "nltk_sw = read_txt('stopwords/english') # save nltk stopword list in variable\n",
    "nltk_sw = clean_word_tokenize(nltk_sw)  # tokenize nltk stopword list\n",
    "\n",
    "# apply stopword lists\n",
    "\n",
    "# OBS! test\n",
    "test1 = ['i', 'am', 'a', 'muslim', 'who', 'loves', 'allah']\n",
    "# virker\n",
    "test1_nosw = [w for w in test1 if not w in sw] \n",
    "# virker\n",
    "test1_nosw = []\n",
    "for w in test1: \n",
    "    if w not in sw: \n",
    "        test1_nosw.append(w)\n",
    "\n",
    "\n",
    "# virker slet ikke\n",
    "nosw_articles_tokens = [w for w in articles_tokens if not w in sw] \n",
    "sw_articles_tokens = [] \n",
    "for w in articles_tokens: \n",
    "    if w not in sw: \n",
    "        sw_articles_tokens.append(w) \n",
    "\n",
    "# virker lidt, men på en mærkelig måde med kun den første artikel som bare duplikeres.\n",
    "nosw_articles_tokens=[]\n",
    "for article in articles_tokens:\n",
    "    nosw_articles=[]\n",
    "    for word in article:\n",
    "        if word not in sw:\n",
    "            nosw_articles.append(word)\n",
    "        nosw_articles_tokens.append(nosw_articles)\n",
    "\n",
    "# apply nltk_sw\n",
    "\n",
    "# test\n",
    "test2 = ['i', 'am', 'a', 'muslim', 'who', 'loves', 'allah']\n",
    "# virker\n",
    "test2_nosw = [w for w in test2 if not w in nltk_sw] \n",
    "# virker\n",
    "test2_nosw = []\n",
    "for w in test2: \n",
    "    if w not in nltk_sw: \n",
    "        test2_nosw.append(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open stemmer downloaded from nltk and apply to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency og evt. distribution plot af udvalgte ord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling (men lige nu uden at stopwords er fjernet og uden pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(articles_tokens)\n",
    "print(dictionary.num_docs)\n",
    "\n",
    "\n",
    "# create bag-of-words representation of the articles\n",
    "text_bow = [dictionary.doc2bow(article) for article in articles_tokens]\n",
    "\n",
    "# 10 topics\n",
    "k = 10\n",
    "mdl = models.LdaModel(text_bow, id2word = dictionary, num_topics = k, random_state = 1234)\n",
    "\n",
    "for i in range(k):\n",
    "    print('Topic',i)\n",
    "    print([t[0] for t in mdl.show_topic(i,10)])\n",
    "    print('-----')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency modeling (association rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
