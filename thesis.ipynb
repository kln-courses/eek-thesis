{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files sorted on filename ##\n",
    "\n",
    "Here we import your .txt files and their filenames. We sort the filenames to keep the order of your index.\n",
    "\n",
    "nb. you need to install the `natsort` module: https://anaconda.org/anaconda/natsort\n",
    "    , which you can either do from the Anaconda Prompt or from the Anaconda Navigator.\n",
    "    If you use the prompt write: `conda install natsort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from natsort import natsorted\n",
    "\n",
    "def read_txt_dir(dirpath):\n",
    "    \"\"\" import all .txt files from directory of directory path dirpath\n",
    "        - output file and filename in list\n",
    "    \"\"\"\n",
    "    filenames = natsorted(glob.glob(os.path.join(dirpath,\"*.txt\")))\n",
    "    files = list()\n",
    "    for filename in filenames:\n",
    "        with open(filename,\"r\") as fobj:\n",
    "            files.append(fobj.read())\n",
    "    filenames = [filename.split(\"/\")[-1] for filename in filenames]\n",
    "    return files, filenames \n",
    "\n",
    "# import articles\n",
    "article_path = os.path.join(\"dat\",\"articles\")\n",
    "articles, article_names = read_txt_dir(article_path)\n",
    "\n",
    "# import magazines\n",
    "magazine_path = os.path.join(\"dat\",\"magazines\")\n",
    "magazines, magazine_names = read_txt_dir(magazine_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and basic corpus statistics ##\n",
    "\n",
    "Start by computing basic corpus statistics. For this you need several functions for preprocessing your string data. We use `re` to remove punctuation and `NLTK` for tokenization. The functionality can be implemented with `re` alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## TOKENIZE ###########################################\n",
    "\n",
    "import re\n",
    "\n",
    "# function to tokenize and lowercase strings\n",
    "def tokenize(input, length = 0, casefold = True):   # ignore tokens shorter than or equal to 3\n",
    "    tokenizer = re.compile(r'[^A-Za-z]+')           # only retain chars\n",
    "    if casefold:                                    # lowercase if casefold = True\n",
    "        input = input.lower()\n",
    "    tokens = [token for token in tokenizer.split(input) if len(token) > length]\n",
    "    return tokens\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "articles_tokens = list(map(tokenize,articles))      # tokenize all articles in list\n",
    "tokens = sorted(flatten(articles_tokens))           # all tokens in one (flat) sorted list\n",
    "n_tokens = len(tokens)                              # total number of tokens\n",
    "n_types = len(list(set(tokens)))                    # number of unique tokens\n",
    "\n",
    "print(\"The corpus consist of {} tokens distributed over {} lexical types\".format(n_tokens, n_types))\n",
    "print(\"The lexical richness measured as the type-token ratio is {}\".format(round(n_types/n_tokens,4)))\n",
    "print(\"On average every word is repeated {} times\".format(round(n_tokens/n_types,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "# function to generate stopword list from dataset with n number of stopwords\n",
    "def gen_ls_stoplist(input, n = 100):\n",
    "    t_f_total = defaultdict(int)\n",
    "    for text in input:\n",
    "        for token in text:\n",
    "            t_f_total[token] += 1\n",
    "    nmax = sorted( t_f_total.items(), key = itemgetter(1), reverse = True)[:n]\n",
    "    return [elem[0] for elem in nmax]\n",
    "\n",
    "# generate stopword list from articles with 50 stopwords\n",
    "sw = gen_ls_stoplist(articles_tokens, 50)\n",
    "\n",
    "import io\n",
    "\n",
    "# function to read txt-file and store content in string\n",
    "def read_txt(filepath):\n",
    "    f = io.open(filepath, 'r', encoding = 'utf-8')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    return content\n",
    "\n",
    "nltksw = read_txt('Stopwords/english')      # save nltk stopword list in variable\n",
    "#nltksw = read_txt('Stopwords/english.txt') ### NOTICE THAT I HAVE CHANGE THE FILE PATH\n",
    "nltksw = tokenize(nltksw)                   # tokenize nltk stopword list\n",
    "\n",
    "# apply sw\n",
    "no_sw = list()\n",
    "for article in articles_tokens:\n",
    "    out = [token for token in article if token not in sw]\n",
    "    no_sw.append(out)\n",
    "\n",
    "# apply nltksw\n",
    "no_nltksw = list()\n",
    "for article in articles_tokens:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    no_nltksw.append(out)\n",
    "\n",
    "# apply sw + nltksw\n",
    "no_swall = list()\n",
    "for article in no_sw:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    no_swall.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 3 char tokens ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### REMOVE 3 CHAR TOKENS #######################################\n",
    "\n",
    "\n",
    "# remove 3 char tokens from articles without sw\n",
    "clean_nosw = list()\n",
    "for article in no_sw:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_nosw.append(out)\n",
    "\n",
    "# remove 3 char tokens from articles without nltksw\n",
    "clean_nonltksw = list()\n",
    "for article in no_nltksw:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_nonltksw.append(out)\n",
    "\n",
    "# remove 3 char tokens from articles without sw + nltk\n",
    "clean_noswall = list()\n",
    "for article in no_swall:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_noswall.append(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### STEMMING  #############################################\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# stemming no sw\n",
    "stem_nosw = list()\n",
    "for article in clean_nosw:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nosw.append(out)\n",
    "\n",
    "# stemming no nltk\n",
    "stem_nonltksw = list()\n",
    "for article in clean_nonltksw:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nonltksw.append(out)\n",
    "\n",
    "# stemming no sw all\n",
    "stem_noswall = list()\n",
    "for article in clean_noswall:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_noswall.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### NEW CORPUS STATISTICS ########################################\n",
    "\n",
    "\n",
    "clean_tokens = sorted(flatten(stem_noswall))            # all tokens in one (flat) sorted list\n",
    "n_clean_tokens = len(clean_tokens)                      # total number of tokens\n",
    "n_clean_types = len(list(set(clean_tokens)))            # number of unique tokens\n",
    "\n",
    "\n",
    "print(\"The corpus now consist of {} tokens distributed over {} lexical types\".format(n_clean_tokens, n_clean_types))\n",
    "print(\"The new lexical richness measured as the type-token ratio is now {}\".format(round(n_clean_types/n_clean_tokens,4)))\n",
    "print(\"On average every word is now repeated {} times\".format(round(n_clean_tokens/n_clean_types,2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# word frequency of entire dataset\n",
    "wf_all = Counter(flatten(articles_tokens))\n",
    "\n",
    "# wf of stem no sw all\n",
    "wf_stem_noswall = Counter(flatten(stem_noswall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pos_tag to tag parts of speech\n",
    "from nltk.tag import pos_tag\n",
    "# tagging tokens\n",
    "tag_all_tokens = pos_tag(tokens, tagset = 'universal', lang = 'eng')\n",
    "\n",
    "# create list with articles only with noun tokens\n",
    "article_nouns = list()\n",
    "for article in articles:\n",
    "    tokenz = tokenize(article, length = 3, casefold = False)        \n",
    "    tagset = pos_tag(tokenz, tagset = 'universal', lang = 'eng')    # tag tokenz\n",
    "    tokenz = [tag[0] for tag in tagset if tag[1] in ['NOUN']]       # only retain nouns\n",
    "    tokenz = [token.lower() for token in tokenz]                    # lowercase\n",
    "    article_nouns.append(tokenz)\n",
    "\n",
    "# STOPWORDS\n",
    "    \n",
    "# apply sw\n",
    "nouns_no_sw = list()\n",
    "for article in article_nouns:\n",
    "    out = [token for token in article if token not in sw]\n",
    "    nouns_no_sw.append(out)\n",
    "\n",
    "# apply sw + nltksw\n",
    "nouns_no_swall = list()\n",
    "for article in nouns_no_sw:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    nouns_no_swall.append(out)\n",
    "\n",
    "# STEMMING    \n",
    "\n",
    "# stemming nouns_no_swall\n",
    "stem_nouns_noswall = list()\n",
    "for article in nouns_no_swall:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nouns_noswall.append(out)\n",
    "\n",
    "\n",
    "# B-O-W    \n",
    "        \n",
    "from gensim import corpora\n",
    "\n",
    "# create dictionary of stem_noswall\n",
    "dic_stem_noswall = corpora.Dictionary(stem_noswall)\n",
    "# use dictionary to create bag of words representation of stem_noswall\n",
    "bow_stem_noswall = [dic_stem_noswall.doc2bow(article) for article in stem_noswall]\n",
    "\n",
    "\n",
    "# create dictionary of nouns no sw all\n",
    "dic_nouns_noswall = corpora.Dictionary(stem_nouns_noswall)\n",
    "# use dictionary to create bag of words representation of articles_nouns\n",
    "bow_nouns_noswall = [dic_nouns_noswall.doc2bow(article) for article in stem_nouns_noswall]\n",
    "\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# create topics of bow_stem_noswall\n",
    "k = 50\n",
    "mdl_noswall = models.LdaModel(bow_stem_noswall, id2word = dic_stem_noswall, num_topics = k, random_state = 1234, iterations = 100, passes = 100)\n",
    "# print topics\n",
    "for i in range(k):\n",
    "    print('Topic',i)\n",
    "    print([t[0] for t in mdl_noswall.show_topic(i,15)])\n",
    "    print('-----')\n",
    "\n",
    "# create topics of bow_nouns_noswall\n",
    "k = 50\n",
    "mdl_nouns_noswall = models.LdaModel(bow_nouns_noswall, id2word = dic_nouns_noswall, num_topics = k, random_state = 1234, iterations = 100, passes = 100)\n",
    "# print topics\n",
    "for i in range(k):\n",
    "    print('Topic',i)\n",
    "    print([t[0] for t in mdl_nouns_noswall.show_topic(i,15)])\n",
    "    print('-----')\n",
    "\n",
    "\n",
    "# EXPLORE THE MODELS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# function to pair topics with articles\n",
    "def get_theta(doc_bow, mdl):\n",
    "    tmp = mdl.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    return [p[1] for p in tmp]\n",
    "\n",
    "# make empty dataframe\n",
    "topics_stem_noswall = pd.DataFrame() \n",
    "# pair topics and articles for stem_noswall\n",
    "for topicnr in range(k):\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topic_score = []\n",
    "    print(topicnr)\n",
    "    for article in range(len(articles)):\n",
    "        topic_score.append(get_theta(bow_stem_noswall[article], mdl_noswall)[topicnr])\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topics_stem_noswall[topic_name] = topic_score\n",
    "    \n",
    "# make empty dataframe\n",
    "topics_nouns_noswall = pd.DataFrame() \n",
    "# pair topics and articles for nouns_noswall\n",
    "for topicnr in range(k):\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topic_score = []\n",
    "    print(topicnr)\n",
    "    for article in range(len(articles)):\n",
    "        topic_score.append(get_theta(bow_nouns_noswall[article], mdl_nouns_noswall)[topicnr])\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topics_nouns_noswall[topic_name] = topic_score  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "def size(obj):\n",
    "    return \"{0:.2f} MB\".format(sys.getsizeof(obj) / (1000 * 1000))\n",
    "\n",
    "# function to return frequency counts for items and item pairs\n",
    "def freq(iterable):\n",
    "    if type(iterable) == pd.core.series.Series:\n",
    "        return iterable.value_counts().rename(\"freq\")\n",
    "    else: \n",
    "        return pd.Series(Counter(iterable)).rename(\"freq\")\n",
    "\n",
    "# function to return number of unique orders\n",
    "def order_count(order_item):\n",
    "    return len(set(order_item.index))\n",
    "\n",
    "# function to return generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    order_item = order_item.reset_index().as_matrix()\n",
    "    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "              \n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "\n",
    "# function to return frequency and support associated with item\n",
    "def merge_item_stats(item_pairs, item_stats):\n",
    "    return (item_pairs\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n",
    "\n",
    "# function to return name associated with item\n",
    "def merge_item_name(rules, item_name):\n",
    "    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n",
    "               'confidenceAtoB','confidenceBtoA','lift']\n",
    "    rules = (rules\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n",
    "    return rules[columns]              \n",
    "    \n",
    "\n",
    "\n",
    "def association_rules(order_item, min_support):\n",
    "\n",
    "    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Calculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Filter from order_item items below min support \n",
    "    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n",
    "    order_item             = order_item[order_item.isin(qualifying_items)]\n",
    "\n",
    "    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Filter from order_item orders with less than 2 items\n",
    "    order_size             = freq(order_item.index)\n",
    "    qualifying_orders      = order_size[order_size >= 2].index\n",
    "    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n",
    "\n",
    "    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Recalculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Get item pairs generator\n",
    "    item_pair_gen          = get_item_pairs(order_item)\n",
    "\n",
    "\n",
    "    # Calculate item pair frequency and support\n",
    "    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n",
    "    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) * 100\n",
    "\n",
    "    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Filter from item_pairs those below min support\n",
    "    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n",
    "\n",
    "    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Create table of association rules and compute relevant metrics\n",
    "    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    item_pairs = merge_item_stats(item_pairs, item_stats)\n",
    "    \n",
    "    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n",
    "    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n",
    "    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n",
    "    \n",
    "    \n",
    "    # Return association rules sorted by lift in descending order\n",
    "    return item_pairs.sort_values('lift', ascending=False)\n",
    "\n",
    "# create dataframe with articles, words\n",
    "df = pd.DataFrame(stem_noswall)\n",
    "# transposing the data for easier handling in R\n",
    "df = df.transpose()\n",
    "# writing to csv-file\n",
    "df.to_csv('stem_noswall.csv')\n",
    "\n",
    "''' work in R to fit dataframe suitable for code'''\n",
    "\n",
    "# import the file created in R\n",
    "orders = pd.read_csv('/Users/emmaelisabethkiis/Desktop/eek-thesis-master/AR_stem_noswall.csv')\n",
    "\n",
    "# preparing file for the association rules function\n",
    "orders = orders.set_index('order_id')['product_id'].rename('item_id')\n",
    "\n",
    "\n",
    "#magic!\n",
    "rules = association_rules(orders, 0.2)  \n",
    "\n",
    "print(rules.head())\n",
    "\n",
    "print('dimensions: {0};   size: {1};   unique_orders: {2};   unique_items: {3}'.format(orders.shape, size(orders), len(orders.index.unique()), len(orders.value_counts())))\n",
    "\n",
    "\n",
    "woman = rules[rules['item_A'].str.match('woman')]\n",
    "man = rules[rules['item_A'].str.match('man')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word associations based on Cosine distance between word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    \"\"\" cosine similarity between vectors a and b\n",
    "    \"\"\"\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "\n",
    "# vector space\n",
    "no_features = 1000# max number of features/words\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "TDM = vectorizer.fit_transform(articles)\n",
    "lexicon = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# compute distance matrix from cosine similarity\n",
    "X = TDM.todense() \n",
    "n = len(lexicon)\n",
    "distance_matrix = np.zeros((n,n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        x1 = X[:,i].ravel().tolist()[0]\n",
    "        x2 = X[:,j].ravel().tolist()[0]\n",
    "        distance_matrix[i,j] = 1 - cosine_similarity(x1,x2)\n",
    "        \n",
    "# write matrix to file\n",
    "cos_df = pd.DataFrame(distance_matrix)\n",
    "cos_df.columns = lexicon[:n]\n",
    "cos_df.index = lexicon[:n]\n",
    "cos_df.to_csv(\"dat/cosine_distance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of query the distance matrix\n",
    "\n",
    "## load the stored distance matrix\n",
    "cos_df = pd.read_csv(\"dat/cosine_distance.csv\",header=0,index_col=0)\n",
    "\n",
    "## define query\n",
    "q1 = \"sisters\"\n",
    "q2 = \"jihad\"\n",
    "\n",
    "## extract distance score\n",
    "print(\"cosine distance between {} and {} is {}\".format(q1,q2,cos_df.loc[q1,q2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
