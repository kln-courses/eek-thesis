{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files sorted on filename ##\n",
    "\n",
    "Here we import your .txt files and their filenames. We sort the filenames to keep the order of your index.\n",
    "\n",
    "nb. you need to install the `natsort` module: https://anaconda.org/anaconda/natsort\n",
    "    , which you can either do from the Anaconda Prompt or from the Anaconda Navigator.\n",
    "    If you use the prompt write: `conda install natsort`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from natsort import natsorted\n",
    "\n",
    "def read_txt_dir(dirpath):\n",
    "    \"\"\" import all .txt files from directory of directory path dirpath\n",
    "        - output file and filename in list\n",
    "    \"\"\"\n",
    "    filenames = natsorted(glob.glob(os.path.join(dirpath,\"*.txt\")))\n",
    "    files = list()\n",
    "    for filename in filenames:\n",
    "        with open(filename,\"r\") as fobj:\n",
    "            files.append(fobj.read())\n",
    "    filenames = [filename.split(\"/\")[-1] for filename in filenames]\n",
    "    return files, filenames \n",
    "\n",
    "# import articles\n",
    "article_path = os.path.join(\"dat\",\"articles\")\n",
    "articles, article_names = read_txt_dir(article_path)\n",
    "\n",
    "# import magazines\n",
    "magazine_path = os.path.join(\"dat\",\"magazines\")\n",
    "magazines, magazine_names = read_txt_dir(magazine_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and basic corpus statistics ##\n",
    "\n",
    "Start by computing basic corpus statistics. For this you need several functions for preprocessing your string data. We use `re` to remove punctuation and `NLTK` for tokenization. The functionality can be implemented with `re` alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus consist of 289006 tokens distributed over 12336 lexical types\n",
      "The lexical richness measured as the type-token ratio is 0.0427\n",
      "On average every word is repeated 23.43 times\n"
     ]
    }
   ],
   "source": [
    "######################################## TOKENIZE ###########################################\n",
    "\n",
    "import re\n",
    "\n",
    "# function to tokenize and lowercase strings\n",
    "def tokenize(input, length = 0, casefold = True):   # ignore tokens shorter than or equal to 3\n",
    "    tokenizer = re.compile(r'[^A-Za-z]+')           # only retain chars\n",
    "    if casefold:                                    # lowercase if casefold = True\n",
    "        input = input.lower()\n",
    "    tokens = [token for token in tokenizer.split(input) if len(token) > length]\n",
    "    return tokens\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "articles_tokens = list(map(tokenize,articles))      # tokenize all articles in list\n",
    "tokens = sorted(flatten(articles_tokens))           # all tokens in one (flat) sorted list\n",
    "n_tokens = len(tokens)                              # total number of tokens\n",
    "n_types = len(list(set(tokens)))                    # number of unique tokens\n",
    "\n",
    "print(\"The corpus consist of {} tokens distributed over {} lexical types\".format(n_tokens, n_types))\n",
    "print(\"The lexical richness measured as the type-token ratio is {}\".format(round(n_types/n_tokens,4)))\n",
    "print(\"On average every word is repeated {} times\".format(round(n_tokens/n_types,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "# function to generate stopword list from dataset with n number of stopwords\n",
    "def gen_ls_stoplist(input, n = 100):\n",
    "    t_f_total = defaultdict(int)\n",
    "    for text in input:\n",
    "        for token in text:\n",
    "            t_f_total[token] += 1\n",
    "    nmax = sorted( t_f_total.items(), key = itemgetter(1), reverse = True)[:n]\n",
    "    return [elem[0] for elem in nmax]\n",
    "\n",
    "# generate stopword list from articles with 50 stopwords\n",
    "sw = gen_ls_stoplist(articles_tokens, 50)\n",
    "\n",
    "import io\n",
    "\n",
    "# function to read txt-file and store content in string\n",
    "def read_txt(filepath):\n",
    "    f = io.open(filepath, 'r', encoding = 'utf-8')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    return content\n",
    "\n",
    "nltksw = read_txt('Stopwords/english')      # save nltk stopword list in variable\n",
    "nltksw = tokenize(nltksw)                   # tokenize nltk stopword list\n",
    "\n",
    "# apply sw\n",
    "no_sw = list()\n",
    "for article in articles_tokens:\n",
    "    out = [token for token in article if token not in sw]\n",
    "    no_sw.append(out)\n",
    "\n",
    "# apply nltksw\n",
    "no_nltksw = list()\n",
    "for article in articles_tokens:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    no_nltksw.append(out)\n",
    "\n",
    "# apply sw + nltksw\n",
    "no_swall = list()\n",
    "for article in no_sw:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    no_swall.append(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 3 char tokens ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### REMOVE 3 CHAR TOKENS #######################################\n",
    "\n",
    "\n",
    "# remove 3 char tokens from articles without sw\n",
    "clean_nosw = list()\n",
    "for article in no_sw:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_nosw.append(out)\n",
    "\n",
    "# remove 3 char tokens from articles without nltksw\n",
    "clean_nonltksw = list()\n",
    "for article in no_nltksw:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_nonltksw.append(out)\n",
    "\n",
    "# remove 3 char tokens from articles without sw + nltk\n",
    "clean_noswall = list()\n",
    "for article in no_swall:\n",
    "    out = [token for token in article if len(token)>3]\n",
    "    clean_noswall.append(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### STEMMING  #############################################\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# stemming no sw\n",
    "stem_nosw = list()\n",
    "for article in clean_nosw:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nosw.append(out)\n",
    "\n",
    "# stemming no nltk\n",
    "stem_nonltksw = list()\n",
    "for article in clean_nonltksw:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nonltksw.append(out)\n",
    "\n",
    "# stemming no sw all\n",
    "stem_noswall = list()\n",
    "for article in clean_noswall:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_noswall.append(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New corpus statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus now consist of 117260 tokens distributed over 7490 lexical types\n",
      "The new lexical richness measured as the type-token ratio is now 0.0639\n",
      "On average every word is now repeated 15.66 times\n"
     ]
    }
   ],
   "source": [
    "################################### NEW CORPUS STATISTICS ########################################\n",
    "\n",
    "\n",
    "clean_tokens = sorted(flatten(stem_noswall))            # all tokens in one (flat) sorted list\n",
    "n_clean_tokens = len(clean_tokens)                      # total number of tokens\n",
    "n_clean_types = len(list(set(clean_tokens)))            # number of unique tokens\n",
    "\n",
    "\n",
    "print(\"The corpus now consist of {} tokens distributed over {} lexical types\".format(n_clean_tokens, n_clean_types))\n",
    "print(\"The new lexical richness measured as the type-token ratio is now {}\".format(round(n_clean_types/n_clean_tokens,4)))\n",
    "print(\"On average every word is now repeated {} times\".format(round(n_clean_tokens/n_clean_types,2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# word frequency of entire dataset\n",
    "wf_all = Counter(flatten(articles_tokens))\n",
    "\n",
    "# wf of stem no sw all\n",
    "wf_stem_noswall = Counter(flatten(stem_noswall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "['religion', 'establish', 'prophet', 'believ', 'islam', 'among', 'know', 'continu', 'muslim', 'call', 'upon', 'claim', 'except', 'order', 'enemi']\n",
      "-----\n",
      "Topic 1\n",
      "['imam', 'sunnah', 'knowledg', 'warn', 'religion', 'marisi', 'scholar', 'muslim', 'name', 'speak', 'weak', 'narrat', 'take', 'jahmiyyah', 'bishr']\n",
      "-----\n",
      "Topic 2\n",
      "['wealth', 'messeng', 'permiss', 'imam', 'kuffar', 'ghanimah', 'secur', 'grant', 'opinion', 'take', 'khum', 'muslim', 'statement', 'scholar', 'prophet']\n",
      "-----\n",
      "Topic 3\n",
      "['vehicl', 'attack', 'oper', 'target', 'locat', 'kuffar', 'larg', 'nusayriyyah', 'forc', 'advertis', 'take', 'tadmur', 'victim', 'weapon', 'import']\n",
      "-----\n",
      "Topic 4\n",
      "['path', 'establish', 'experi', 'hypothes', 'result', 'religion', 'theori', 'movement', 'walk', 'straight', 'follow', 'reach', 'goal', 'upright', 'work']\n",
      "-----\n",
      "Topic 5\n",
      "['kill', 'soldier', 'citi', 'murtaddin', 'area', 'khilafah', 'nusayri', 'vehicl', 'explos', 'armi', 'attack', 'rafidi', 'number', 'took', 'rabi']\n",
      "-----\n",
      "Topic 6\n",
      "['father', 'asma', 'husband', 'khadijah', 'bakr', 'mother', 'famili', 'sumayyah', 'wealth', 'ammar', 'tortur', 'steadfast', 'kill', 'religion', 'pass']\n",
      "-----\n",
      "Topic 7\n",
      "['pledg', 'fulfil', 'make', 'patienc', 'steadfast', 'flee', 'muslim', 'report', 'coven', 'messeng', 'battl', 'upon', 'islam', 'death', 'emir']\n",
      "-----\n",
      "Topic 8\n",
      "['imam', 'muslim', 'islam', 'claim', 'principl', 'appoint', 'religion', 'believ', 'rafidah', 'upon', 'state', 'obey', 'inde', 'imamah', 'even']\n",
      "-----\n",
      "Topic 9\n",
      "['kill', 'messeng', 'women', 'muslim', 'home', 'also', 'soldier', 'knowledg', 'companion', 'prophet', 'citi', 'murtaddin', 'mushrikin', 'took', 'woman']\n",
      "-----\n",
      "Topic 10\n",
      "['brother', 'return', 'say', 'face', 'medic', 'enemi', 'find', 'diwan', 'place', 'health', 'turn', 'sweet', 'shaykh', 'permiss', 'battl']\n",
      "-----\n",
      "Topic 11\n",
      "['mean', 'ilaha', 'illallah', 'word', 'heart', 'categori', 'imag', 'shall', 'mention', 'without', 'statement', 'wit', 'water', 'muslim', 'report']\n",
      "-----\n",
      "Topic 12\n",
      "['soldier', 'kill', 'murtaddin', 'khilafah', 'citi', 'crusad', 'vehicl', 'explos', 'armi', 'murtadd', 'wound', 'area', 'state', 'attack', 'villag']\n",
      "-----\n",
      "Topic 13\n",
      "['messeng', 'upon', 'muslim', 'believ', 'make', 'prophet', 'report', 'inde', 'lord', 'islam', 'victori', 'heart', 'say', 'time', 'come']\n",
      "-----\n",
      "Topic 14\n",
      "['pledg', 'fulfil', 'coven', 'flee', 'break', 'muslim', 'crusad', 'death', 'report', 'enemi', 'islam', 'sahabah', 'combat', 'mean', 'make']\n",
      "-----\n",
      "Topic 15\n",
      "['believ', 'messeng', 'enemi', 'inde', 'muslim', 'ahzab', 'take', 'upon', 'islam', 'land', 'promis', 'prophet', 'among', 'khilafah', 'condit']\n",
      "-----\n",
      "Topic 16\n",
      "['believ', 'muslim', 'jihad', 'also', 'describ', 'report', 'children', 'except', 'trait', 'caus', 'nifaq', 'messeng', 'munafiqin', 'increas', 'wage']\n",
      "-----\n",
      "Topic 17\n",
      "['bless', 'grate', 'lord', 'mean', 'whoever', 'among', 'favor', 'shaykh', 'jama', 'upon', 'home', 'islam', 'mention', 'could', 'gratitud']\n",
      "-----\n",
      "Topic 18\n",
      "['attack', 'fallujah', 'fire', 'bottl', 'build', 'target', 'gasolin', 'battl', 'molotov', 'son', 'cocktail', 'contain', 'american', 'piec', 'area']\n",
      "-----\n",
      "Topic 19\n",
      "['burn', 'test', 'kill', 'brother', 'fire', 'offic', 'heart', 'children', 'turkish', 'abdillah', 'islam', 'tawaghit', 'howev', 'call', 'person']\n",
      "-----\n",
      "Topic 20\n",
      "['muslim', 'kill', 'fight', 'blood', 'islam', 'coven', 'permiss', 'give', 'kuffar', 'report', 'messeng', 'kafir', 'land', 'rule', 'prophet']\n",
      "-----\n",
      "Topic 21\n",
      "['prophet', 'test', 'islam', 'muslim', 'support', 'inde', 'messeng', 'believ', 'truth', 'battl', 'except', 'fight', 'religion', 'women', 'upon']\n",
      "-----\n",
      "Topic 22\n",
      "['mujahidin', 'brother', 'baqarah', 'protect', 'jihad', 'quran', 'call', 'sabah', 'corrupt', 'munafiqin', 'time', 'remain', 'battl', 'among', 'land']\n",
      "-----\n",
      "Topic 23\n",
      "['worship', 'seek', 'judgment', 'rule', 'lord', 'judg', 'whoever', 'mean', 'taghut', 'believ', 'reveal', 'tawhid', 'messeng', 'shirk', 'except']\n",
      "-----\n",
      "Topic 24\n",
      "['wealth', 'kuffar', 'secur', 'permiss', 'muslim', 'imam', 'prophet', 'take', 'khum', 'harbi', 'grant', 'blood', 'land', 'islam', 'among']\n",
      "-----\n",
      "Topic 25\n",
      "['muslim', 'kill', 'messeng', 'prophet', 'battl', 'report', 'women', 'enemi', 'islam', 'victori', 'fight', 'roman', 'wealth', 'jihad', 'khalid']\n",
      "-----\n",
      "Topic 26\n",
      "['brother', 'islam', 'believ', 'franc', 'muslim', 'make', 'upon', 'mujahid', 'hijrah', 'order', 'target', 'lighter', 'caus', 'skill', 'prepar']\n",
      "-----\n",
      "Topic 27\n",
      "['fear', 'knowledg', 'likewis', 'ignor', 'statement', 'necessit', 'admonish', 'envis', 'believ', 'rememb', 'like', 'remind', 'case', 'know', 'seek']\n",
      "-----\n",
      "Topic 28\n",
      "['muslim', 'suffer', 'defeat', 'kill', 'continu', 'mushrikin', 'everi', 'wealth', 'jihad', 'battl', 'reward', 'eventu', 'religion', 'fight', 'number']\n",
      "-----\n",
      "Topic 29\n",
      "['messeng', 'prophet', 'islam', 'jihad', 'siyahah', 'report', 'imam', 'among', 'believ', 'muslim', 'upon', 'religion', 'kill', 'battl', 'inde']\n",
      "-----\n",
      "Topic 30\n",
      "['muslim', 'crusad', 'prophet', 'bless', 'brother', 'kill', 'health', 'report', 'harm', 'bodi', 'good', 'khilafah', 'oper', 'bengali', 'mani']\n",
      "-----\n",
      "Topic 31\n",
      "['islam', 'state', 'make', 'takfir', 'kufr', 'mushrikin', 'faction', 'refrain', 'claim', 'matter', 'muslim', 'religion', 'fight', 'establish', 'shari']\n",
      "-----\n",
      "Topic 32\n",
      "['mansur', 'tariqah', 'lodg', 'sufi', 'follow', 'sinai', 'shirk', 'also', 'mujahidin', 'answer', 'tariqa', 'soothsay', 'question', 'tawhid', 'enemi']\n",
      "-----\n",
      "Topic 33\n",
      "['yasin', 'citi', 'islam', 'area', 'tadmur', 'abdullah', 'bakr', 'umar', 'brother', 'kill', 'jihad', 'religion', 'nusayri', 'tribe', 'mujahidin']\n",
      "-----\n",
      "Topic 34\n",
      "['islam', 'crusad', 'fight', 'inde', 'religion', 'land', 'enemi', 'battl', 'support', 'upon', 'mujahidin', 'truth', 'caus', 'lord', 'state']\n",
      "-----\n",
      "Topic 35\n",
      "['muslim', 'wealth', 'imam', 'kill', 'state', 'upon', 'take', 'religion', 'prophet', 'messeng', 'land', 'fight', 'coven', 'rule', 'attack']\n",
      "-----\n",
      "Topic 36\n",
      "['injustic', 'case', 'investig', 'offic', 'wrong', 'grievanc', 'state', 'soldier', 'islam', 'occur', 'bodi', 'elimin', 'judgment', 'establish', 'leader']\n",
      "-----\n",
      "Topic 37\n",
      "['mujahidin', 'base', 'oper', 'forc', 'unit', 'form', 'establish', 'tawhid', 'uniti', 'everi', 'call', 'muslim', 'khilafah', 'word', 'banner']\n",
      "-----\n",
      "Topic 38\n",
      "['day', 'prophet', 'deed', 'muslim', 'jihad', 'among', 'religion', 'islam', 'messeng', 'imam', 'follow', 'report', 'perform', 'establish', 'upon']\n",
      "-----\n",
      "Topic 39\n",
      "['muslim', 'report', 'believ', 'upon', 'fight', 'prophet', 'inde', 'support', 'except', 'kill', 'call', 'victori', 'lord', 'heart', 'enemi']\n",
      "-----\n",
      "Topic 40\n",
      "['emir', 'obey', 'steadfast', 'call', 'report', 'listen', 'troop', 'messeng', 'person', 'muslim', 'flee', 'abba', 'patienc', 'hunayn', 'tulaqa']\n",
      "-----\n",
      "Topic 41\n",
      "['husband', 'wife', 'women', 'backbit', 'spain', 'spous', 'woman', 'secret', 'speak', 'even', 'home', 'tongu', 'instead', 'word', 'find']\n",
      "-----\n",
      "Topic 42\n",
      "['islam', 'imam', 'state', 'religion', 'rule', 'establish', 'rafidah', 'claim', 'brother', 'upon', 'yasin', 'could', 'follow', 'muslim', 'began']\n",
      "-----\n",
      "Topic 43\n",
      "['muharram', 'attack', 'karkuk', 'citi', 'toward', 'vest', 'midst', 'grenad', 'process', 'brother', 'larg', 'shirki', 'hand', 'council', 'peshmerga']\n",
      "-----\n",
      "Topic 44\n",
      "['women', 'masjid', 'prayer', 'woman', 'prophet', 'report', 'home', 'masajid', 'muslim', 'messeng', 'prevent', 'narrat', 'hous', 'leav', 'pray']\n",
      "-----\n",
      "Topic 45\n",
      "['knowledg', 'time', 'scholar', 'upon', 'book', 'truth', 'deed', 'report', 'action', 'muslim', 'jihad', 'without', 'pillar', 'thu', 'bless']\n",
      "-----\n",
      "Topic 46\n",
      "['scholar', 'conceal', 'evil', 'truth', 'tawaghit', 'knowledg', 'follow', 'inde', 'jihad', 'path', 'dunya', 'curs', 'clear', 'mean', 'musa']\n",
      "-----\n",
      "Topic 47\n",
      "['idol', 'muslim', 'indian', 'sultan', 'worship', 'kill', 'ibnul', 'destroy', 'mahmud', 'messeng', 'took', 'athir', 'ghaznawi', 'dawlah', 'yamin']\n",
      "-----\n",
      "Topic 48\n",
      "['celebr', 'mushrikin', 'target', 'attack', 'kill', 'place', 'muslim', 'knife', 'month', 'oper', 'mujahidin', 'strike', 'caus', 'blade', 'disbeliev']\n",
      "-----\n",
      "Topic 49\n",
      "['husband', 'siyahah', 'woman', 'marri', 'remarri', 'women', 'among', 'report', 'children', 'hadith', 'die', 'prophet', 'ummah', 'monastic', 'fast']\n",
      "-----\n",
      "Topic 0\n",
      "['nusayri', 'area', 'citi', 'armi', 'tadmur', 'state', 'islam', 'sahwat', 'regim', 'sahwah', 'nusayriyyah', 'sham', 'airbas', 'turkey', 'control']\n",
      "-----\n",
      "Topic 1\n",
      "['time', 'believ', 'messeng', 'victori', 'patienc', 'support', 'deed', 'lord', 'hardship', 'dunya', 'hereaft', 'life', 'trial', 'jannah', 'test']\n",
      "-----\n",
      "Topic 2\n",
      "['locat', 'oper', 'target', 'victim', 'advertis', 'forc', 'crusad', 'hostag', 'firearm', 'properti', 'number', 'muhammad', 'attack', 'method', 'exampl']\n",
      "-----\n",
      "Topic 3\n",
      "['soldier', 'citi', 'khilafah', 'murtaddin', 'vehicl', 'oper', 'crusad', 'number', 'area', 'state', 'wilayah', 'islam', 'armi', 'rafidi', 'other']\n",
      "-----\n",
      "Topic 4\n",
      "['islam', 'muslim', 'religion', 'brother', 'sunnah', 'caus', 'khilafah', 'messeng', 'hand', 'prophet', 'enemi', 'sham', 'land', 'lord', 'crusad']\n",
      "-----\n",
      "Topic 5\n",
      "['religion', 'sunnah', 'imam', 'methodolog', 'knowledg', 'justic', 'muslim', 'sect', 'marisi', 'scholar', 'messeng', 'name', 'bishr', 'mistak', 'jahmiyyah']\n",
      "-----\n",
      "Topic 6\n",
      "['husband', 'woman', 'women', 'children', 'mother', 'prophet', 'famili', 'lord', 'father', 'sister', 'jannah', 'muslim', 'dunya', 'bint', 'messeng']\n",
      "-----\n",
      "Topic 7\n",
      "['societi', 'state', 'sahabah', 'prophet', 'muslim', 'islam', 'condit', 'aspect', 'messeng', 'companion', 'regard', 'oppress', 'realiti', 'number', 'battl']\n",
      "-----\n",
      "Topic 8\n",
      "['crusad', 'muslim', 'oper', 'bengali', 'khilafah', 'bengal', 'islam', 'soldier', 'brother', 'aqil', 'prophet', 'garden', 'roman', 'hand', 'ana']\n",
      "-----\n",
      "Topic 9\n",
      "['pledg', 'muslim', 'coven', 'victori', 'condit', 'chapter', 'prophet', 'break', 'report', 'flee', 'soldier', 'sahabah', 'fulfil', 'death', 'murtaddin']\n",
      "-----\n",
      "Topic 10\n",
      "['chariti', 'wealth', 'women', 'messeng', 'jihad', 'soul', 'caus', 'muslim', 'prophet', 'whoever', 'thing', 'wage', 'armi', 'islam', 'fear']\n",
      "-----\n",
      "Topic 11\n",
      "['crusad', 'islam', 'state', 'shari', 'citi', 'lord', 'soldier', 'enemi', 'sirt', 'muslim', 'nation', 'murtadd', 'govern', 'mujahidin', 'khilafah']\n",
      "-----\n",
      "Topic 12\n",
      "['messeng', 'heart', 'women', 'time', 'prophet', 'believ', 'sunnah', 'muslim', 'quran', 'hous', 'disbeliev', 'life', 'prayer', 'islam', 'baqarah']\n",
      "-----\n",
      "Topic 13\n",
      "['brother', 'muhammad', 'shaykh', 'islam', 'caus', 'messeng', 'death', 'religion', 'adnani', 'companion', 'enemi', 'soldier', 'prophet', 'grace', 'disbeliev']\n",
      "-----\n",
      "Topic 14\n",
      "['knowledg', 'citi', 'soldier', 'rabi', 'murtaddin', 'state', 'vehicl', 'posit', 'rafidi', 'other', 'imam', 'scholar', 'likewis', 'khilafah', 'quran']\n",
      "-----\n",
      "Topic 15\n",
      "['muslim', 'messeng', 'prophet', 'bukhari', 'time', 'patienc', 'kufr', 'islam', 'hurayrah', 'report', 'anyon', 'ibnul', 'order', 'hunayn', 'abba']\n",
      "-----\n",
      "Topic 16\n",
      "['movement', 'taliban', 'relat', 'presenc', 'state', 'border', 'afghanistan', 'faction', 'organ', 'khurasan', 'islam', 'land', 'central', 'exchang', 'fate']\n",
      "-----\n",
      "Topic 17\n",
      "['muslim', 'messeng', 'islam', 'enemi', 'religion', 'prophet', 'victori', 'lord', 'battl', 'land', 'heart', 'number', 'believ', 'state', 'disbeliev']\n",
      "-----\n",
      "Topic 18\n",
      "['day', 'deed', 'muslim', 'dhul', 'hijjah', 'tatar', 'year', 'taymiyyah', 'ahzab', 'messeng', 'jihad', 'time', 'period', 'place', 'confeder']\n",
      "-----\n",
      "Topic 19\n",
      "['women', 'woman', 'prayer', 'home', 'masjid', 'prophet', 'masajid', 'hous', 'muslim', 'wive', 'umar', 'time', 'someon', 'prevent', 'rule']\n",
      "-----\n",
      "Topic 20\n",
      "['religion', 'imam', 'path', 'state', 'theori', 'rafidah', 'rule', 'islam', 'mahdi', 'order', 'follow', 'imamah', 'prophet', 'condit', 'leadership']\n",
      "-----\n",
      "Topic 21\n",
      "['yasin', 'umar', 'islam', 'bakr', 'abdullah', 'shari', 'salawi', 'knife', 'land', 'religion', 'murabitin', 'oper', 'someon', 'maghrib', 'tribe']\n",
      "-----\n",
      "Topic 22\n",
      "['abdillah', 'injuri', 'ramadan', 'month', 'toward', 'burn', 'join', 'halab', 'action', 'kufr', 'someth', 'critic', 'fight', 'shahadah', 'other']\n",
      "-----\n",
      "Topic 23\n",
      "['messeng', 'book', 'sunnah', 'judgment', 'heart', 'someth', 'statement', 'religion', 'ahmad', 'other', 'believ', 'obedi', 'lord', 'matter', 'word']\n",
      "-----\n",
      "Topic 24\n",
      "['fear', 'knowledg', 'statement', 'likewis', 'person', 'case', 'quran', 'rememb', 'necessit', 'someon', 'reason', 'ignor', 'scholar', 'heart', 'exampl']\n",
      "-----\n",
      "Topic 25\n",
      "['muslim', 'islam', 'imam', 'muhammad', 'principl', 'state', 'religion', 'time', 'place', 'other', 'sulayman', 'order', 'shaykh', 'celebr', 'month']\n",
      "-----\n",
      "Topic 26\n",
      "['vehicl', 'attack', 'oper', 'target', 'load', 'method', 'enemi', 'speed', 'truck', 'weapon', 'frame', 'islam', 'car', 'carnag', 'outer']\n",
      "-----\n",
      "Topic 27\n",
      "['soldier', 'khilafah', 'vehicl', 'murtaddin', 'citi', 'area', 'dhul', 'crusad', 'rafidi', 'number', 'clash', 'villag', 'posit', 'other', 'shawwal']\n",
      "-----\n",
      "Topic 28\n",
      "['vehicl', 'citi', 'soldier', 'khilafah', 'villag', 'safar', 'state', 'islam', 'oper', 'tank', 'dhul', 'brother', 'wilayah', 'attack', 'armi']\n",
      "-----\n",
      "Topic 29\n",
      "['muslim', 'islam', 'imam', 'land', 'messeng', 'kuffar', 'religion', 'prophet', 'battl', 'bukhari', 'bless', 'regard', 'case', 'blood', 'harbi']\n",
      "-----\n",
      "Topic 30\n",
      "['knowledg', 'heart', 'truth', 'scholar', 'quran', 'baqarah', 'religion', 'time', 'word', 'action', 'path', 'sunnah', 'other', 'believ', 'corrupt']\n",
      "-----\n",
      "Topic 31\n",
      "['rabi', 'awwal', 'control', 'airbas', 'missil', 'tank', 'hill', 'mujahidin', 'group', 'tadmur', 'addit', 'battalion', 'shami', 'checkpoint', 'jihar']\n",
      "-----\n",
      "Topic 32\n",
      "['messeng', 'muslim', 'trait', 'nifaq', 'news', 'jihad', 'permiss', 'time', 'faction', 'hypocrit', 'believ', 'caus', 'heart', 'hypocrisi', 'stingi']\n",
      "-----\n",
      "Topic 33\n",
      "['soldier', 'islam', 'khilafah', 'area', 'citi', 'muslim', 'vehicl', 'prophet', 'women', 'crusad', 'armi', 'posit', 'rafidi', 'fear', 'state']\n",
      "-----\n",
      "Topic 34\n",
      "['islam', 'muslim', 'lord', 'religion', 'rule', 'worship', 'messeng', 'prophet', 'pledg', 'partner', 'judgment', 'judg', 'pillar', 'shirk', 'statement']\n",
      "-----\n",
      "Topic 35\n",
      "['islam', 'state', 'religion', 'muslim', 'messeng', 'prophet', 'bless', 'matter', 'lord', 'land', 'imam', 'statement', 'hijrah', 'author', 'evid']\n",
      "-----\n",
      "Topic 36\n",
      "['muslim', 'wealth', 'blood', 'prophet', 'messeng', 'kuffar', 'secur', 'imam', 'islam', 'coven', 'land', 'permiss', 'children', 'scholar', 'opinion']\n",
      "-----\n",
      "Topic 37\n",
      "['siyahah', 'prophet', 'monastic', 'hadith', 'likewis', 'hijrah', 'messeng', 'salaf', 'food', 'quran', 'worship', 'tabari', 'caus', 'heart', 'permit']\n",
      "-----\n",
      "Topic 38\n",
      "['islam', 'refrain', 'claim', 'akhirah', 'other', 'matter', 'wife', 'messeng', 'memorandum', 'muslim', 'husband', 'faction', 'women', 'shari', 'term']\n",
      "-----\n",
      "Topic 39\n",
      "['crusad', 'answer', 'question', 'islam', 'state', 'east', 'asia', 'region', 'mujahidin', 'time', 'khilafah', 'weapon', 'soldier', 'battl', 'goal']\n",
      "-----\n",
      "Topic 40\n",
      "['messeng', 'muslim', 'prophet', 'asim', 'islam', 'bukhari', 'thabit', 'worship', 'malik', 'hassan', 'umar', 'strength', 'mushrik', 'companion', 'bakr']\n",
      "-----\n",
      "Topic 41\n",
      "['day', 'khadijah', 'children', 'father', 'asma', 'hijjah', 'deed', 'sumayyah', 'religion', 'jihad', 'lord', 'dhul', 'bukhari', 'wealth', 'prophet']\n",
      "-----\n",
      "Topic 42\n",
      "['time', 'messeng', 'brother', 'women', 'dunya', 'islam', 'heart', 'sister', 'husband', 'muslim', 'desir', 'hereaft', 'role', 'hardship', 'toward']\n",
      "-----\n",
      "Topic 43\n",
      "['muslim', 'messeng', 'caus', 'battl', 'time', 'jihad', 'believ', 'brother', 'hypocrit', 'trait', 'news', 'nifaq', 'wealth', 'permiss', 'heart']\n",
      "-----\n",
      "Topic 44\n",
      "['children', 'muslim', 'heart', 'statement', 'word', 'prophet', 'women', 'illallah', 'idol', 'islam', 'mention', 'categori', 'ilaha', 'imag', 'messeng']\n",
      "-----\n",
      "Topic 45\n",
      "['battl', 'brother', 'muslim', 'islam', 'time', 'enemi', 'messeng', 'victori', 'mujahidin', 'state', 'lord', 'heart', 'event', 'weapon', 'steadfast']\n",
      "-----\n",
      "Topic 46\n",
      "['case', 'injustic', 'islam', 'offic', 'state', 'grievanc', 'soldier', 'bodi', 'investig', 'sens', 'muslim', 'leader', 'obligatori', 'anyon', 'regard']\n",
      "-----\n",
      "Topic 47\n",
      "['book', 'messeng', 'scholar', 'retribut', 'fire', 'hadith', 'eye', 'head', 'bukhari', 'soldier', 'person', 'muslim', 'ibnul', 'evid', 'prophet']\n",
      "-----\n",
      "Topic 48\n",
      "['idol', 'indian', 'sultan', 'muslim', 'ibnul', 'mahmud', 'attack', 'ghaznawi', 'fire', 'bottl', 'dawlah', 'athir', 'yamin', 'gasolin', 'india']\n",
      "-----\n",
      "Topic 49\n",
      "['islam', 'brother', 'mansur', 'mujahid', 'path', 'scholar', 'order', 'state', 'religion', 'famili', 'sufi', 'sinai', 'lodg', 'truth', 'tariqah']\n",
      "-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# import pos_tag to tag parts of speech\n",
    "from nltk.tag import pos_tag\n",
    "# tagging tokens\n",
    "tag_all_tokens = pos_tag(tokens, tagset = 'universal', lang = 'eng')\n",
    "\n",
    "# create list with articles only with noun tokens\n",
    "article_nouns = list()\n",
    "for article in articles:\n",
    "    tokenz = tokenize(article, length = 3, casefold = False)        \n",
    "    tagset = pos_tag(tokenz, tagset = 'universal', lang = 'eng')    # tag tokenz\n",
    "    tokenz = [tag[0] for tag in tagset if tag[1] in ['NOUN']]       # only retain nouns\n",
    "    tokenz = [token.lower() for token in tokenz]                    # lowercase\n",
    "    article_nouns.append(tokenz)\n",
    "\n",
    "# STOPWORDS\n",
    "    \n",
    "# apply sw\n",
    "nouns_no_sw = list()\n",
    "for article in article_nouns:\n",
    "    out = [token for token in article if token not in sw]\n",
    "    nouns_no_sw.append(out)\n",
    "\n",
    "# apply sw + nltksw\n",
    "nouns_no_swall = list()\n",
    "for article in nouns_no_sw:\n",
    "    out = [token for token in article if token not in nltksw]\n",
    "    nouns_no_swall.append(out)\n",
    "\n",
    "# STEMMING    \n",
    "\n",
    "# stemming nouns_no_swall\n",
    "stem_nouns_noswall = list()\n",
    "for article in nouns_no_swall:\n",
    "    out = [ps.stem(token) for token in article]\n",
    "    stem_nouns_noswall.append(out)\n",
    "\n",
    "\n",
    "# B-O-W    \n",
    "        \n",
    "from gensim import corpora\n",
    "\n",
    "# create dictionary of stem_noswall\n",
    "dic_stem_noswall = corpora.Dictionary(stem_noswall)\n",
    "# use dictionary to create bag of words representation of stem_noswall\n",
    "bow_stem_noswall = [dic_stem_noswall.doc2bow(article) for article in stem_noswall]\n",
    "\n",
    "\n",
    "# create dictionary of nouns no sw all\n",
    "dic_nouns_noswall = corpora.Dictionary(stem_nouns_noswall)\n",
    "# use dictionary to create bag of words representation of articles_nouns\n",
    "bow_nouns_noswall = [dic_nouns_noswall.doc2bow(article) for article in stem_nouns_noswall]\n",
    "\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# create topics of bow_stem_noswall\n",
    "k = 50\n",
    "mdl_noswall = models.LdaModel(bow_stem_noswall, id2word = dic_stem_noswall, num_topics = k, random_state = 1234, iterations = 100, passes = 100)\n",
    "# print topics\n",
    "for i in range(k):\n",
    "    print('Topic',i)\n",
    "    print([t[0] for t in mdl_noswall.show_topic(i,15)])\n",
    "    print('-----')\n",
    "\n",
    "# create topics of bow_nouns_noswall\n",
    "k = 50\n",
    "mdl_nouns_noswall = models.LdaModel(bow_nouns_noswall, id2word = dic_nouns_noswall, num_topics = k, random_state = 1234, iterations = 100, passes = 100)\n",
    "# print topics\n",
    "for i in range(k):\n",
    "    print('Topic',i)\n",
    "    print([t[0] for t in mdl_nouns_noswall.show_topic(i,15)])\n",
    "    print('-----')\n",
    "\n",
    "\n",
    "# EXPLORE THE MODELS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# function to pair topics with articles\n",
    "def get_theta(doc_bow, mdl):\n",
    "    tmp = mdl.get_document_topics(doc_bow, minimum_probability=0)\n",
    "    return [p[1] for p in tmp]\n",
    "\n",
    "# make empty dataframe\n",
    "topics_stem_noswall = pd.DataFrame() \n",
    "# pair topics and articles for stem_noswall\n",
    "for topicnr in range(k):\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topic_score = []\n",
    "    print(topicnr)\n",
    "    for article in range(len(articles)):\n",
    "        topic_score.append(get_theta(bow_stem_noswall[article], mdl_noswall)[topicnr])\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topics_stem_noswall[topic_name] = topic_score\n",
    "    \n",
    "# make empty dataframe\n",
    "topics_nouns_noswall = pd.DataFrame() \n",
    "# pair topics and articles for nouns_noswall\n",
    "for topicnr in range(k):\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topic_score = []\n",
    "    print(topicnr)\n",
    "    for article in range(len(articles)):\n",
    "        topic_score.append(get_theta(bow_nouns_noswall[article], mdl_nouns_noswall)[topicnr])\n",
    "    topic_name = 'topic %d' %topicnr\n",
    "    topics_nouns_noswall[topic_name] = topic_score  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting order_item:                 363320\n",
      "Items with support >= 0.2:            7490\n",
      "Remaining order_item:                117260\n",
      "Remaining orders with 2+ items:         124\n",
      "Remaining order_item:                117260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmaelisabethkiis/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item pairs:                         8372926\n",
      "Item pairs with support >= 0.2:    8372926\n",
      "\n",
      "          item_A     item_B  freqAB  supportAB  freqA  supportA  freqB  \\\n",
      "923175   allaqah       poem       1   0.806452      1  0.806452      1   \n",
      "7799709    subtl  telegraph       1   0.806452      1  0.806452      1   \n",
      "7831743   athram  telegraph       2   1.612903      2  1.612903      1   \n",
      "7826892    hulay  telegraph       1   0.806452      1  0.806452      1   \n",
      "7822522  hamasah  telegraph       1   0.806452      1  0.806452      1   \n",
      "\n",
      "         supportB  confidenceAtoB  confidenceBtoA  lift  \n",
      "923175   0.806452             1.0             1.0  1.24  \n",
      "7799709  0.806452             1.0             1.0  1.24  \n",
      "7831743  0.806452             1.0             2.0  1.24  \n",
      "7826892  0.806452             1.0             1.0  1.24  \n",
      "7822522  0.806452             1.0             1.0  1.24  \n",
      "dimensions: (363320,);   size: 21.24 MB;   unique_orders: 124;   unique_items: 7490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "\n",
    "def size(obj):\n",
    "    return \"{0:.2f} MB\".format(sys.getsizeof(obj) / (1000 * 1000))\n",
    "\n",
    "# function to return frequency counts for items and item pairs\n",
    "def freq(iterable):\n",
    "    if type(iterable) == pd.core.series.Series:\n",
    "        return iterable.value_counts().rename(\"freq\")\n",
    "    else: \n",
    "        return pd.Series(Counter(iterable)).rename(\"freq\")\n",
    "\n",
    "# function to return number of unique orders\n",
    "def order_count(order_item):\n",
    "    return len(set(order_item.index))\n",
    "\n",
    "# function to return generator that yields item pairs, one at a time\n",
    "def get_item_pairs(order_item):\n",
    "    order_item = order_item.reset_index().as_matrix()\n",
    "    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "              \n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "\n",
    "# function to return frequency and support associated with item\n",
    "def merge_item_stats(item_pairs, item_stats):\n",
    "    return (item_pairs\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n",
    "                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True))\n",
    "\n",
    "# function to return name associated with item\n",
    "def merge_item_name(rules, item_name):\n",
    "    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n",
    "               'confidenceAtoB','confidenceBtoA','lift']\n",
    "    rules = (rules\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemA'}), left_on='item_A', right_on='item_id')\n",
    "                .merge(item_name.rename(columns={'item_name': 'itemB'}), left_on='item_B', right_on='item_id'))\n",
    "    return rules[columns]              \n",
    "    \n",
    "\n",
    "\n",
    "def association_rules(order_item, min_support):\n",
    "\n",
    "    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Calculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Filter from order_item items below min support \n",
    "    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n",
    "    order_item             = order_item[order_item.isin(qualifying_items)]\n",
    "\n",
    "    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Filter from order_item orders with less than 2 items\n",
    "    order_size             = freq(order_item.index)\n",
    "    qualifying_orders      = order_size[order_size >= 2].index\n",
    "    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n",
    "\n",
    "    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n",
    "    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n",
    "\n",
    "\n",
    "    # Recalculate item frequency and support\n",
    "    item_stats             = freq(order_item).to_frame(\"freq\")\n",
    "    item_stats['support']  = item_stats['freq'] / order_count(order_item) * 100\n",
    "\n",
    "\n",
    "    # Get item pairs generator\n",
    "    item_pair_gen          = get_item_pairs(order_item)\n",
    "\n",
    "\n",
    "    # Calculate item pair frequency and support\n",
    "    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n",
    "    item_pairs['supportAB'] = item_pairs['freqAB'] / len(qualifying_orders) * 100\n",
    "\n",
    "    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Filter from item_pairs those below min support\n",
    "    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n",
    "\n",
    "    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n",
    "\n",
    "\n",
    "    # Create table of association rules and compute relevant metrics\n",
    "    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    item_pairs = merge_item_stats(item_pairs, item_stats)\n",
    "    \n",
    "    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] / item_pairs['supportA']\n",
    "    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] / item_pairs['supportB']\n",
    "    item_pairs['lift']           = item_pairs['supportAB'] / (item_pairs['supportA'] * item_pairs['supportB'])\n",
    "    \n",
    "    \n",
    "    # Return association rules sorted by lift in descending order\n",
    "    return item_pairs.sort_values('lift', ascending=False)\n",
    "\n",
    "# create dataframe with articles, words\n",
    "df = pd.DataFrame(stem_noswall)\n",
    "# transposing the data for easier handling in R\n",
    "df = df.transpose()\n",
    "# writing to csv-file\n",
    "df.to_csv('stem_noswall.csv')\n",
    "\n",
    "''' work in R to fit dataframe suitable for code'''\n",
    "\n",
    "# import the file created in R\n",
    "orders = pd.read_csv('/Users/emmaelisabethkiis/Desktop/eek-thesis-master/AR_stem_noswall.csv')\n",
    "\n",
    "# preparing file for the association rules function\n",
    "orders = orders.set_index('order_id')['product_id'].rename('item_id')\n",
    "\n",
    "\n",
    "#magic!\n",
    "rules = association_rules(orders, 0.2)  \n",
    "\n",
    "print(rules.head())\n",
    "\n",
    "print('dimensions: {0};   size: {1};   unique_orders: {2};   unique_items: {3}'.format(orders.shape, size(orders), len(orders.index.unique()), len(orders.value_counts())))\n",
    "\n",
    "\n",
    "woman = rules[rules['item_A'].str.match('woman')]\n",
    "man = rules[rules['item_A'].str.match('man')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
